{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [1. Results](#results)\n",
    "* [2. Import libraries and data](#import)\n",
    "    * [2.1 Data dictionary](#dict)\n",
    "* [3. Preprocessing](#preprocessing)\n",
    "* [4. Feature Engineering](#feature)\n",
    "* [5. Modelling](#model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Results <a class=\"anchor\" id=\"results\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veggies es bonus vobis, proinde vos postulo essum magis kohlrabi welsh onion daikon amaranth tatsoi tomatillo melon azuki bean garlic.\n",
    "\n",
    "Gumbo beet greens corn soko endive gumbo gourd. Parsley shallot courgette tatsoi pea sprouts fava bean collard greens dandelion okra wakame tomato. Dandelion cucumber earthnut pea peanut soko zucchini.\n",
    "\n",
    "Turnip greens yarrow ricebean rutabaga endive cauliflower sea lettuce kohlrabi amaranth water spinach avocado daikon napa cabbage asparagus winter purslane kale. Celery potato scallion desert raisin horseradish spinach carrot soko. Lotus root water spinach fennel kombu maize bamboo shoot green bean swiss chard seakale pumpkin onion chickpea gram corn pea. Brussels sprout coriander water chestnut gourd swiss chard wakame kohlrabi beetroot carrot watercress. Corn amaranth salsify bunya nuts nori azuki bean chickweed potato bell pepper artichoke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multi-label:\n",
    "    - https://machinelearningmastery.com/multi-label-classification-with-deep-learning/\n",
    "    - https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries and data <a class=\"anchor\" id=\"import\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading files\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Data cleaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer  # remove punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # to create document-term matrices from X_train and X_test\n",
    "\n",
    "# Model utility\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "# We cannot use SMOTE for multi-class: Need to do 1 vs all\n",
    "# https://www.kaggle.com/questions-and-answers/93669\n",
    "\n",
    "# Modelling\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../data')\n",
    "\n",
    "# df1 = pd.read_csv(\"dataset_1.csv\", encoding=\"latin1\")\n",
    "\n",
    "# with open('MMHS150K_GT.json', encoding=\"utf8\") as json_file:\n",
    "#     json_file = json.load(json_file)\n",
    "    \n",
    "df = pd.read_csv(\"full_df.csv\", encoding=\"latin1\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>NotHate</th>\n",
       "      <th>Racist</th>\n",
       "      <th>Sexist</th>\n",
       "      <th>Homophobe</th>\n",
       "      <th>Religion</th>\n",
       "      <th>OtherHate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@FriskDontMiss Nigga https://t.co/cAsaLWEpue</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My horses are retarded https://t.co/HYhqc6d5WN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>âNIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>âEVERYbody calling you Nigger now!â https:...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  NotHate  Racist  Sexist  \\\n",
       "0       @FriskDontMiss Nigga https://t.co/cAsaLWEpue        0       1       0   \n",
       "1     My horses are retarded https://t.co/HYhqc6d5WN        0       0       0   \n",
       "2  âNIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL...        1       0       0   \n",
       "3  RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...        1       1       0   \n",
       "4  âEVERYbody calling you Nigger now!â https:...        1       1       0   \n",
       "\n",
       "   Homophobe  Religion  OtherHate  \n",
       "0          1         1          0  \n",
       "1          0         0          1  \n",
       "2          0         0          0  \n",
       "3          0         0          0  \n",
       "4          0         0          0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rename(columns = {\n",
    "    \"id\": \"id\",\n",
    "    \"Tweets\": \"tweets1\",\n",
    "    \"Label\": \"label1\"\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_url': 'http://pbs.twimg.com/tweet_video_thumb/D3gi9MHWAAAgfl7.jpg',\n",
       " 'labels': [4, 1, 3],\n",
       " 'tweet_url': 'https://twitter.com/user/status/1114679353714016256',\n",
       " 'tweet_text': '@FriskDontMiss Nigga https://t.co/cAsaLWEpue',\n",
       " 'labels_str': ['Religion', 'Racist', 'Homophobe']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file['1114679353714016256']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_todf(json):\n",
    "    \"\"\"\n",
    "    Convert JSON data into dataframe, using by mapping as follows:\n",
    "    id: json key\n",
    "    Tweets: tweet_text\n",
    "    Labels (list of int): labels\n",
    "    Labels (list of str): labels_str\n",
    "    \"\"\"\n",
    "    res = {\"id\": [], \"tweets\": [], \"label\": [], \"label_str\": []}\n",
    "    \n",
    "    for key, value in json.items():\n",
    "        res[\"id\"].append(key)\n",
    "        res[\"tweets\"].append(value[\"tweet_text\"])\n",
    "        res[\"label\"].append(value[\"labels\"])\n",
    "        res[\"label_str\"].append(value[\"labels_str\"])\n",
    "        \n",
    "    df = pd.DataFrame(res)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1114679353714016256</td>\n",
       "      <td>@FriskDontMiss Nigga https://t.co/cAsaLWEpue</td>\n",
       "      <td>[4, 1, 3]</td>\n",
       "      <td>[Religion, Racist, Homophobe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1063020048816660480</td>\n",
       "      <td>My horses are retarded https://t.co/HYhqc6d5WN</td>\n",
       "      <td>[5, 5, 5]</td>\n",
       "      <td>[OtherHate, OtherHate, OtherHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1108927368075374593</td>\n",
       "      <td>“NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL S...</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[NotHate, NotHate, NotHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1114558534635618305</td>\n",
       "      <td>RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>[Racist, NotHate, NotHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1035252480215592966</td>\n",
       "      <td>“EVERYbody calling you Nigger now!” https://t....</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "      <td>[Racist, NotHate, Racist]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                             tweets  \\\n",
       "0  1114679353714016256       @FriskDontMiss Nigga https://t.co/cAsaLWEpue   \n",
       "1  1063020048816660480     My horses are retarded https://t.co/HYhqc6d5WN   \n",
       "2  1108927368075374593  “NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL S...   \n",
       "3  1114558534635618305  RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...   \n",
       "4  1035252480215592966  “EVERYbody calling you Nigger now!” https://t....   \n",
       "\n",
       "       label                          label_str  \n",
       "0  [4, 1, 3]      [Religion, Racist, Homophobe]  \n",
       "1  [5, 5, 5]  [OtherHate, OtherHate, OtherHate]  \n",
       "2  [0, 0, 0]        [NotHate, NotHate, NotHate]  \n",
       "3  [1, 0, 0]         [Racist, NotHate, NotHate]  \n",
       "4  [1, 0, 1]          [Racist, NotHate, Racist]  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = convert_json_todf(json_file)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16035 149823 165858\n"
     ]
    }
   ],
   "source": [
    "print(len(df1), len(df2), len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>NotHate</th>\n",
       "      <th>Racist</th>\n",
       "      <th>Sexist</th>\n",
       "      <th>Homophobe</th>\n",
       "      <th>Religion</th>\n",
       "      <th>OtherHate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@FriskDontMiss Nigga https://t.co/cAsaLWEpue</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My horses are retarded https://t.co/HYhqc6d5WN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>âNIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>âEVERYbody calling you Nigger now!â https:...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>â real ass bitch give a fuck boutta niggaâ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@WhiteHouse @realDonaldTrump Fuck ice. White s...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dayâs a cunt https://t.co/Ie6QZReHsw</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#sissy faggot https://t.co/bm1nk8HcYO</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@Gloriko_ Nigga what? https://t.co/nOwIJtgtU1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@DefNotJerm So.... you turn to twitter for it ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@WhatUpJT I swear I was waiting for her to mou...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Iâm ð¯ behind you nigga u my thug brotherð...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Who dafuqq is this nigga https://t.co/D6YwVyGjNZ</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bf: move your bighead ð",
       "ð  gf: NIGGA WHAT...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@OriginalSlimC This fat nigga slander is getti...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>#ecofascist @SierraClub is just another nest f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>When a android nigga wanna talk about phone ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@FreedomzWriterx Check out this niggerð­ htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Dat nigga my homie 3ï¸â£ https://t.co/rO9DYg...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweets  NotHate  Racist  \\\n",
       "0        @FriskDontMiss Nigga https://t.co/cAsaLWEpue        0       1   \n",
       "1      My horses are retarded https://t.co/HYhqc6d5WN        0       0   \n",
       "2   âNIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL...        1       0   \n",
       "3   RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...        1       1   \n",
       "4   âEVERYbody calling you Nigger now!â https:...        1       1   \n",
       "5   â real ass bitch give a fuck boutta niggaâ...        1       0   \n",
       "6   @WhiteHouse @realDonaldTrump Fuck ice. White s...        0       1   \n",
       "7              Dayâs a cunt https://t.co/Ie6QZReHsw        1       0   \n",
       "8               #sissy faggot https://t.co/bm1nk8HcYO        1       0   \n",
       "9       @Gloriko_ Nigga what? https://t.co/nOwIJtgtU1        1       0   \n",
       "10  @DefNotJerm So.... you turn to twitter for it ...        1       0   \n",
       "11  @WhatUpJT I swear I was waiting for her to mou...        1       0   \n",
       "12  Iâm ð¯ behind you nigga u my thug brotherð...        1       0   \n",
       "13   Who dafuqq is this nigga https://t.co/D6YwVyGjNZ        1       1   \n",
       "14  bf: move your bighead ð\n",
       "ð  gf: NIGGA WHAT...        0       1   \n",
       "15  @OriginalSlimC This fat nigga slander is getti...        1       0   \n",
       "16  #ecofascist @SierraClub is just another nest f...        1       0   \n",
       "17  When a android nigga wanna talk about phone ba...        1       0   \n",
       "18  @FreedomzWriterx Check out this niggerð­ htt...        1       0   \n",
       "19  Dat nigga my homie 3ï¸â£ https://t.co/rO9DYg...        1       0   \n",
       "\n",
       "    Sexist  Homophobe  Religion  OtherHate  \n",
       "0        0          1         1          0  \n",
       "1        0          0         0          1  \n",
       "2        0          0         0          0  \n",
       "3        0          0         0          0  \n",
       "4        0          0         0          0  \n",
       "5        0          0         0          0  \n",
       "6        0          0         0          1  \n",
       "7        0          0         0          0  \n",
       "8        0          1         0          0  \n",
       "9        0          1         1          0  \n",
       "10       0          0         0          0  \n",
       "11       0          0         0          0  \n",
       "12       0          0         0          0  \n",
       "13       0          0         0          0  \n",
       "14       1          0         0          0  \n",
       "15       0          1         0          0  \n",
       "16       0          0         0          0  \n",
       "17       0          0         0          0  \n",
       "18       0          0         0          0  \n",
       "19       0          0         0          0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)  # we will use this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NotHate classified: 91.75258353531333%\n",
      "Percentage of Racist classified: 31.25625535096287%\n",
      "Percentage of Sexist classified: 13.306563445839211%\n",
      "Percentage of Homophobe classified: 7.340013746698983%\n",
      "Percentage of Religion classified: 1.4596823789024347%\n",
      "Percentage of OtherHate classified: 14.875375321057772%\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of NotHate classified: {}%\".format(df['NotHate'].value_counts()[1] * 100 / len(df)))\n",
    "print(\"Percentage of Racist classified: {}%\".format(df['Racist'].value_counts()[1] * 100 / len(df)))\n",
    "print(\"Percentage of Sexist classified: {}%\".format(df['Sexist'].value_counts()[1] * 100 / len(df)))\n",
    "print(\"Percentage of Homophobe classified: {}%\".format(df['Homophobe'].value_counts()[1] * 100 / len(df)))\n",
    "print(\"Percentage of Religion classified: {}%\".format(df['Religion'].value_counts()[1] * 100 / len(df)))\n",
    "print(\"Percentage of OtherHate classified: {}%\".format(df['OtherHate'].value_counts()[1] * 100 / len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Dictionary <a class=\"anchor\" id=\"dict\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Name|Variable Name| Description\n",
    "|---|:---:|:---\n",
    "|id|id|Unique identifier for each tweet\n",
    "|Tweets|Tweet content|Body of tweet\n",
    "|Label|classification of label|Multi-class label: sexism, racism, homophobe, religion, other hate or none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing <a class=\"anchor\" id=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Add code to combine datasets\n",
    "\n",
    "0. Tokenizing (NLTK)\n",
    "1. Remove stopwords\n",
    "2. Remove links and '@' <-- if using only English words, then not necessary\n",
    "3. Stemming / Lemmatization (https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)\n",
    "4. Add custom hate speech vocab  # decide later\n",
    "5. Dealing with emojis # TODO: later\n",
    "7. Tokenizing (CountVectorizer)\n",
    "\n",
    "TODO: Create custom vocab <br>\n",
    "TODO2: https://stackoverflow.com/questions/28339622/is-there-a-corpora-of-english-words-in-nltk\n",
    "\n",
    "- Reasons for not removing English words: some people like to mispell\n",
    "--> We can try to do word similarities and autocorrect\n",
    "\n",
    "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0221152\n",
    "\n",
    "https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer\n",
    "\n",
    "**Propose**: Similarity matching for mispelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Tokenize (NLTK) and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "stopword_list[:10]  # all in lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aal',\n",
       " 'aalii',\n",
       " 'aam',\n",
       " 'Aani',\n",
       " 'aardvark',\n",
       " 'aardwolf',\n",
       " 'Aaron']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words = words.words()\n",
    "english_words[:10]  # case-sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe and produces a list containing a list of tokenized tweets.\n",
    "    Applies the following filters:\n",
    "    1. Removes words with @\n",
    "    2. (optional) Removes non-English words\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    \n",
    "    stopword_list = stopwords.words(\"english\")\n",
    "    english_words = words.words()\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # remove punctuation    \n",
    "    tokenized = list(map(tokenizer.tokenize, list(df['Tweets'])))\n",
    "    \n",
    "    for tweet in tokenized:\n",
    "        cleaned_tweet = [w for w in tweet if w.lower() not in stopword_list]\n",
    "        # cleaned_tweet = [w for w in cleaned_tweet if w in english_words]  # too slow\n",
    "        \n",
    "        res.append(cleaned_tweet)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['FriskDontMiss', 'Nigga', 'https', 'co', 'cAsaLWEpue'],\n",
       " ['horses', 'retarded', 'https', 'co', 'HYhqc6d5WN'],\n",
       " ['â',\n",
       "  'NIGGA',\n",
       "  'MOMMA',\n",
       "  'YOUNGBOY',\n",
       "  'SPITTING',\n",
       "  'REAL',\n",
       "  'SHIT',\n",
       "  'NIGGAâ',\n",
       "  'https',\n",
       "  'co',\n",
       "  'UczofqHrLq'],\n",
       " ['RT',\n",
       "  'xxSuGVNGxx',\n",
       "  'ran',\n",
       "  'HOLY',\n",
       "  'NIGGA',\n",
       "  'TODAY',\n",
       "  'ð',\n",
       "  'ð',\n",
       "  'ð',\n",
       "  'ð',\n",
       "  'https',\n",
       "  'co',\n",
       "  'Wa6Spl9kIw'],\n",
       " ['â', 'EVERYbody', 'calling', 'Nigger', 'â', 'https', 'co', '6mguJ6KIBF']]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_list = tokenize(df)\n",
    "tokenized_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(lst):\n",
    "    res = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for tweet in lst:\n",
    "        lemmatized_tweet = [wordnet_lemmatizer.lemmatize(w) for w in tweet]\n",
    "        res.append(lemmatized_tweet)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['FriskDontMiss', 'Nigga', 'http', 'co', 'cAsaLWEpue'],\n",
       " ['horse', 'retarded', 'http', 'co', 'HYhqc6d5WN'],\n",
       " ['â',\n",
       "  'NIGGA',\n",
       "  'MOMMA',\n",
       "  'YOUNGBOY',\n",
       "  'SPITTING',\n",
       "  'REAL',\n",
       "  'SHIT',\n",
       "  'NIGGAâ',\n",
       "  'http',\n",
       "  'co',\n",
       "  'UczofqHrLq'],\n",
       " ['RT',\n",
       "  'xxSuGVNGxx',\n",
       "  'ran',\n",
       "  'HOLY',\n",
       "  'NIGGA',\n",
       "  'TODAY',\n",
       "  'ð',\n",
       "  'ð',\n",
       "  'ð',\n",
       "  'ð',\n",
       "  'http',\n",
       "  'co',\n",
       "  'Wa6Spl9kIw'],\n",
       " ['â', 'EVERYbody', 'calling', 'Nigger', 'â', 'http', 'co', '6mguJ6KIBF']]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_list = lemmatize(tokenized_list)\n",
    "lemmatized_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_test = []\n",
    "\n",
    "for idx, ele in enumerate(lemmatized_list):\n",
    "    for i in range(len(ele)):\n",
    "        if lemmatized_list[idx][i] != tokenized_list[idx][i]:\n",
    "            if (lemmatized_list[idx][i], tokenized_list[idx][i]) not in lemma_test:\n",
    "                lemma_test.append((lemmatized_list[idx][i], tokenized_list[idx][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http', 'https'),\n",
       " ('horse', 'horses'),\n",
       " ('as', 'ass'),\n",
       " ('say', 'says'),\n",
       " ('call', 'calls'),\n",
       " ('co', 'cos'),\n",
       " ('racist', 'racists'),\n",
       " ('jena', 'jenas'),\n",
       " ('saving', 'savings'),\n",
       " ('there', 'theres')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Append back to dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue: too much junk words --> high dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Use CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>NotHate</th>\n",
       "      <th>Racist</th>\n",
       "      <th>Sexist</th>\n",
       "      <th>Homophobe</th>\n",
       "      <th>Religion</th>\n",
       "      <th>OtherHate</th>\n",
       "      <th>tokenized_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@FriskDontMiss Nigga https://t.co/cAsaLWEpue</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, FriskDontMiss, Nigga, https, :, //t.co/cAs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My horses are retarded https://t.co/HYhqc6d5WN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[My, horses, are, retarded, https, :, //t.co/H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>âNIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[âNIGGA, ON, MA, MOMMA, YOUNGBOY, BE, SPITTI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[RT, xxSuGVNGxx, :, I, ran, into, this, HOLY, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>âEVERYbody calling you Nigger now!â https:...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[âEVERYbody, calling, you, Nigger, now, !, â...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  NotHate  Racist  Sexist  \\\n",
       "0       @FriskDontMiss Nigga https://t.co/cAsaLWEpue        0       1       0   \n",
       "1     My horses are retarded https://t.co/HYhqc6d5WN        0       0       0   \n",
       "2  âNIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL...        1       0       0   \n",
       "3  RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...        1       1       0   \n",
       "4  âEVERYbody calling you Nigger now!â https:...        1       1       0   \n",
       "\n",
       "   Homophobe  Religion  OtherHate  \\\n",
       "0          1         1          0   \n",
       "1          0         0          1   \n",
       "2          0         0          0   \n",
       "3          0         0          0   \n",
       "4          0         0          0   \n",
       "\n",
       "                                    tokenized_tweets  \n",
       "0  [@, FriskDontMiss, Nigga, https, :, //t.co/cAs...  \n",
       "1  [My, horses, are, retarded, https, :, //t.co/H...  \n",
       "2  [âNIGGA, ON, MA, MOMMA, YOUNGBOY, BE, SPITTI...  \n",
       "3  [RT, xxSuGVNGxx, :, I, ran, into, this, HOLY, ...  \n",
       "4  [âEVERYbody, calling, you, Nigger, now, !, â...  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorizer_tokenize(df, stop_words=None, max_df=1.0, min_df=1.0):\n",
    "    \"\"\"\n",
    "    1. Split data\n",
    "    2. Create document-term matrices for train and test\n",
    "    Code taken from: 04 Natural_Language_Processing using NB\n",
    "    \"\"\"\n",
    "    vect = CountVectorizer(\n",
    "        stop_words=stop_words,\n",
    "        max_df=max_df,\n",
    "        min_df=min_df\n",
    "    )  # flexible parameters\n",
    "    \n",
    "    X = df['Tweets']\n",
    "    y = df.drop(columns = [\"Tweets\", \"tokenized_tweets\"])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "    \n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    \n",
    "    return vect, X_train_dtm, X_test_dtm, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect, X_train_dtm, X_test_dtm, y_train, y_test = count_vectorizer_tokenize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((124393, 221454), (41465, 221454))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm.shape, X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect, X_train_dtm, X_test_dtm, y_train, y_test = count_vectorizer_tokenize(df, \n",
    "                                                                           stop_words=\"english\", max_df=1.0, min_df=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((124393, 6066), (41465, 6066))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm.shape, X_test_dtm.shape  # using 0.0001 we get ~6000 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00', '000', '01', '02', '03', '04', '05', '05pm', '06', '06jank']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[:10]\n",
    "\n",
    "# can remove numbers, gibberish (aalwuhaib1977), userids (allanharper1920)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the prof used 16825 features in the example, we'll try to get around there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect, X_train_dtm, X_test_dtm, y_train, y_test = count_vectorizer_tokenize(df, \n",
    "                                                                           stop_words=\"english\", \n",
    "                                                                           max_df=1.0, min_df=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((124393, 9920), (41465, 9920))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm.shape, X_test_dtm.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect, X_train_dtm, X_test_dtm, y_train, y_test = count_vectorizer_tokenize(df, \n",
    "                                                                           stop_words=\"english\", \n",
    "                                                                           max_df=1.0, min_df=0.00003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((124393, 15761), (41465, 15761))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm.shape, X_test_dtm.shape  # 0.00003 looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ø³ù',\n",
       " 'øªù',\n",
       " 'ï¼',\n",
       " '¾ð',\n",
       " '¾ï',\n",
       " '¾â',\n",
       " '½ð',\n",
       " '½ï',\n",
       " '½â',\n",
       " '¼ð',\n",
       " '¼ï',\n",
       " '¼ã',\n",
       " '¼â',\n",
       " 'ºð',\n",
       " 'ºï',\n",
       " 'ºâ',\n",
       " 'ºredneck',\n",
       " '¹ð',\n",
       " '¹ï',\n",
       " '¹â',\n",
       " 'µð',\n",
       " 'µï',\n",
       " 'µâ',\n",
       " '³ó',\n",
       " '³ð',\n",
       " '³ï',\n",
       " '³â',\n",
       " '²ð',\n",
       " 'ªð',\n",
       " 'ªï',\n",
       " 'ªã',\n",
       " 'ªâ',\n",
       " 'zâ',\n",
       " 'zython86',\n",
       " 'zupta_chologist',\n",
       " 'zulu',\n",
       " 'zuckles',\n",
       " 'zoro',\n",
       " 'zoom',\n",
       " 'zoo',\n",
       " 'zones',\n",
       " 'zone',\n",
       " 'zombies',\n",
       " 'zombie',\n",
       " 'zoeydollaz',\n",
       " 'zoey',\n",
       " 'zoe',\n",
       " 'zo',\n",
       " 'zirtun',\n",
       " 'zip_zona',\n",
       " 'zip',\n",
       " 'ziorim',\n",
       " 'zionist',\n",
       " 'zionazi',\n",
       " 'zion',\n",
       " 'zimmerman',\n",
       " 'zigmanfreud',\n",
       " 'zh_ha89',\n",
       " 'zge1s6xz84',\n",
       " 'zeus',\n",
       " 'zesty',\n",
       " 'zero',\n",
       " 'zendayacoochie',\n",
       " 'zebra',\n",
       " 'zealanders',\n",
       " 'zealand',\n",
       " 'zakirnaikirf',\n",
       " 'zaki_safar',\n",
       " 'zainab',\n",
       " 'zaibatsunews',\n",
       " 'zahoorgorsi',\n",
       " 'zackfox',\n",
       " 'zack',\n",
       " 'zach',\n",
       " 'za',\n",
       " 'yâ',\n",
       " 'yywodfargd',\n",
       " 'yvngplank',\n",
       " 'yvettecoopermp',\n",
       " 'yves',\n",
       " 'yusuke',\n",
       " 'yusufpeaceful',\n",
       " 'yuskan0723',\n",
       " 'yup',\n",
       " 'yungill314',\n",
       " 'yungbastard_',\n",
       " 'yung',\n",
       " 'yummy',\n",
       " 'yum',\n",
       " 'yui',\n",
       " 'yuhkyle',\n",
       " 'yuh',\n",
       " 'yugi',\n",
       " 'yuck',\n",
       " 'yu',\n",
       " 'yt',\n",
       " 'yslchain',\n",
       " 'ysl',\n",
       " 'yrs',\n",
       " 'yr',\n",
       " 'yplac',\n",
       " 'ypj',\n",
       " 'ypg',\n",
       " 'yoð',\n",
       " 'yoâ',\n",
       " 'yoyotrav',\n",
       " 'yow',\n",
       " 'youð',\n",
       " 'youâ',\n",
       " 'youx',\n",
       " 'youuuuu',\n",
       " 'youuuu',\n",
       " 'youuu',\n",
       " 'youu',\n",
       " 'youtubers',\n",
       " 'youtuber',\n",
       " 'youtube',\n",
       " 'youth',\n",
       " 'yousufpoosuf',\n",
       " 'youse',\n",
       " 'yous',\n",
       " 'yourâ',\n",
       " 'yoursâ',\n",
       " 'yourrage5',\n",
       " 'yourmusicwhore',\n",
       " 'youre',\n",
       " 'youngâ',\n",
       " 'youngthug',\n",
       " 'youngin',\n",
       " 'youngest',\n",
       " 'younger',\n",
       " 'youngboy',\n",
       " 'young',\n",
       " 'youn',\n",
       " 'youfunnyb',\n",
       " 'youfoundthecard',\n",
       " 'youdontknowheartbreaktill',\n",
       " 'yoshi',\n",
       " 'yoself',\n",
       " 'yorkshire',\n",
       " 'yorks',\n",
       " 'york',\n",
       " 'yoooooo',\n",
       " 'yooooo',\n",
       " 'yoooo',\n",
       " 'yooo',\n",
       " 'yoonistry',\n",
       " 'yoongi',\n",
       " 'yoobin',\n",
       " 'yoo',\n",
       " 'yolo',\n",
       " 'yoh',\n",
       " 'yoga',\n",
       " 'yodetroitdeezy',\n",
       " 'yobbo',\n",
       " 'yo',\n",
       " 'ynwmelly',\n",
       " 'ynw',\n",
       " 'yngmstr_cgn',\n",
       " 'ymeftjtiug',\n",
       " 'yk',\n",
       " 'yikes',\n",
       " 'yhu',\n",
       " 'yh',\n",
       " 'yg',\n",
       " 'yezidis',\n",
       " 'yezidi',\n",
       " 'yew',\n",
       " 'yetð',\n",
       " 'yeti',\n",
       " 'yesyouresexist',\n",
       " 'yesyoureracist',\n",
       " 'yesterdayâ',\n",
       " 'yesterday',\n",
       " 'yessssss',\n",
       " 'yessss',\n",
       " 'yesss',\n",
       " 'yess',\n",
       " 'yesjulz',\n",
       " 'yesallwomen',\n",
       " 'yes',\n",
       " 'yer',\n",
       " 'yep',\n",
       " 'yeonjun',\n",
       " 'yemen',\n",
       " 'yelp',\n",
       " 'yells',\n",
       " 'yellowflashguy',\n",
       " 'yellow',\n",
       " 'yelling',\n",
       " 'yelled',\n",
       " 'yell',\n",
       " 'yeh',\n",
       " 'yeezy',\n",
       " 'yeeyee',\n",
       " 'yeet',\n",
       " 'yeen',\n",
       " 'yeehaw',\n",
       " 'yee',\n",
       " 'years',\n",
       " 'year',\n",
       " 'yeap',\n",
       " 'yeah',\n",
       " 'yeaaah',\n",
       " 'yea',\n",
       " 'ye',\n",
       " 'ycdsb',\n",
       " 'ybn',\n",
       " 'yb',\n",
       " 'yað',\n",
       " 'yaâ',\n",
       " 'yayaswh0',\n",
       " 'yay',\n",
       " 'yawning',\n",
       " 'yawn',\n",
       " 'yasss',\n",
       " 'yashar',\n",
       " 'yas',\n",
       " 'yards',\n",
       " 'yard',\n",
       " 'yanks',\n",
       " 'yankees',\n",
       " 'yankee',\n",
       " 'yang',\n",
       " 'yamiche',\n",
       " 'yam',\n",
       " 'yallð',\n",
       " 'yalls',\n",
       " 'yall',\n",
       " 'yal',\n",
       " 'yakwtfgo',\n",
       " 'yak',\n",
       " 'yahweh',\n",
       " 'yah',\n",
       " 'yacht',\n",
       " 'ya',\n",
       " 'y_alibhai',\n",
       " 'y2h9whp68v',\n",
       " 'xð',\n",
       " 'xâ',\n",
       " 'xxx',\n",
       " 'xxsugvngxx',\n",
       " 'xxl',\n",
       " 'xx',\n",
       " 'xwjzpsodgj',\n",
       " 'xvideos',\n",
       " 'xvaencn5ng',\n",
       " 'xrp',\n",
       " 'xoxo',\n",
       " 'xox',\n",
       " 'xor',\n",
       " 'xo',\n",
       " 'xnqhq3vjh5',\n",
       " 'xmas',\n",
       " 'xjegbn9dlx',\n",
       " 'xihnvevbba',\n",
       " 'xhosa',\n",
       " 'xfrce',\n",
       " 'xfactor',\n",
       " 'xenophobic',\n",
       " 'xd',\n",
       " 'xcx',\n",
       " 'xboxshare',\n",
       " 'xbox',\n",
       " 'xans',\n",
       " 'xan',\n",
       " 'x2',\n",
       " 'wâ',\n",
       " 'wypipo',\n",
       " 'wym',\n",
       " 'wylin',\n",
       " 'wydâ',\n",
       " 'wyd',\n",
       " 'wya',\n",
       " 'wwg1wga',\n",
       " 'wwf',\n",
       " 'wwehof',\n",
       " 'wwegames',\n",
       " 'wwe',\n",
       " 'ww',\n",
       " 'wvjoe911',\n",
       " 'wv',\n",
       " 'wut',\n",
       " 'wussup',\n",
       " 'wu',\n",
       " 'wtkavxiryq',\n",
       " 'wth',\n",
       " 'wtfð',\n",
       " 'wtfvids_',\n",
       " 'wtf',\n",
       " 'wsredneck',\n",
       " 'wsp',\n",
       " 'wsj',\n",
       " 'wrote',\n",
       " 'wrongly',\n",
       " 'wrong',\n",
       " 'wroetoshaw',\n",
       " 'wrld',\n",
       " 'written',\n",
       " 'writing',\n",
       " 'writes',\n",
       " 'writers',\n",
       " 'writer',\n",
       " 'write',\n",
       " 'wrist',\n",
       " 'wrestling',\n",
       " 'wrestler',\n",
       " 'wrestlemania35',\n",
       " 'wrestlemania',\n",
       " 'wrecking',\n",
       " 'wrecked',\n",
       " 'wreck',\n",
       " 'wraps',\n",
       " 'wrapping',\n",
       " 'wrapped',\n",
       " 'wrap',\n",
       " 'wpsgglhqcs',\n",
       " 'wp',\n",
       " 'woyk4gnarg',\n",
       " 'wow',\n",
       " 'wounds',\n",
       " 'wouldâ',\n",
       " 'wouldve',\n",
       " 'wouldnâ',\n",
       " 'wouldnt',\n",
       " 'wouldn',\n",
       " 'woulda',\n",
       " 'wot',\n",
       " 'worthy',\n",
       " 'worthless',\n",
       " 'worth',\n",
       " 'worst',\n",
       " 'worshiping',\n",
       " 'worship',\n",
       " 'worse',\n",
       " 'worrying',\n",
       " 'worry',\n",
       " 'worries',\n",
       " 'worried',\n",
       " 'worn',\n",
       " 'worm',\n",
       " 'worldâ',\n",
       " 'worldwidewob',\n",
       " 'worldwide',\n",
       " 'worldstar',\n",
       " 'worlds',\n",
       " 'worlddevgn',\n",
       " 'world',\n",
       " 'workð',\n",
       " 'workâ',\n",
       " 'works',\n",
       " 'workplace',\n",
       " 'workout',\n",
       " 'working',\n",
       " 'workin',\n",
       " 'workflow',\n",
       " 'workers',\n",
       " 'worker',\n",
       " 'worked',\n",
       " 'work',\n",
       " 'wore',\n",
       " 'wordâ',\n",
       " 'words',\n",
       " 'word',\n",
       " 'woohoo',\n",
       " 'woody',\n",
       " 'woodward',\n",
       " 'woods',\n",
       " 'wood',\n",
       " 'woo',\n",
       " 'wonâ',\n",
       " 'wont',\n",
       " 'wonderwoman',\n",
       " 'wonders',\n",
       " 'wonderland',\n",
       " 'wondering',\n",
       " 'wonderful',\n",
       " 'wondered',\n",
       " 'wonder',\n",
       " 'won',\n",
       " 'womenâ',\n",
       " 'womensmarch',\n",
       " 'womens',\n",
       " 'womenagainstfeminism',\n",
       " 'women',\n",
       " 'womb',\n",
       " 'womanâ',\n",
       " 'womans',\n",
       " 'woman',\n",
       " 'wolves',\n",
       " 'wolverine',\n",
       " 'wolfspirit2013',\n",
       " 'wolf',\n",
       " 'woken',\n",
       " 'woke',\n",
       " 'woc',\n",
       " 'wobble',\n",
       " 'woah',\n",
       " 'wnba',\n",
       " 'wmv',\n",
       " 'wmbnxxr6xx',\n",
       " 'wm',\n",
       " 'wizkhalifa',\n",
       " 'wizardryofozil',\n",
       " 'wizard',\n",
       " 'wiz',\n",
       " 'wives',\n",
       " 'witty',\n",
       " 'witnesses',\n",
       " 'witnessed',\n",
       " 'witness',\n",
       " 'withð',\n",
       " 'withâ',\n",
       " 'withtrish',\n",
       " 'withdraw',\n",
       " 'withajmf',\n",
       " 'witchu',\n",
       " 'witcho',\n",
       " 'witchery',\n",
       " 'witch',\n",
       " 'wit',\n",
       " 'wishlist',\n",
       " 'wishing',\n",
       " 'wishes',\n",
       " 'wished',\n",
       " 'wish',\n",
       " 'wise',\n",
       " 'wisdom',\n",
       " 'wisconsin',\n",
       " 'wired',\n",
       " 'wire',\n",
       " 'wiping',\n",
       " 'wipes',\n",
       " 'wipers',\n",
       " 'wiped',\n",
       " 'wipe',\n",
       " 'winter',\n",
       " 'winston',\n",
       " 'wins',\n",
       " 'winning',\n",
       " 'winners',\n",
       " 'winner',\n",
       " 'winking',\n",
       " 'winitwednesday',\n",
       " 'wings',\n",
       " 'wing',\n",
       " 'wine',\n",
       " 'windycityrenzo',\n",
       " 'windshield',\n",
       " 'windows',\n",
       " 'window',\n",
       " 'wind',\n",
       " 'win',\n",
       " 'wimpy',\n",
       " 'wimme',\n",
       " 'wilytone',\n",
       " 'wilson',\n",
       " 'willy',\n",
       " 'willing',\n",
       " 'willie',\n",
       " 'willian',\n",
       " 'williams',\n",
       " 'william',\n",
       " 'willfully',\n",
       " 'will__ne',\n",
       " 'wilding',\n",
       " 'wildin',\n",
       " 'wildest',\n",
       " 'wild',\n",
       " 'wil',\n",
       " 'wikipedia',\n",
       " 'wikileaks',\n",
       " 'wiki',\n",
       " 'wigger',\n",
       " 'wigan',\n",
       " 'wig',\n",
       " 'wifi',\n",
       " 'wifeâ',\n",
       " 'wifey',\n",
       " 'wifes',\n",
       " 'wife',\n",
       " 'widow',\n",
       " 'wide',\n",
       " 'wid',\n",
       " 'wicked',\n",
       " 'wick',\n",
       " 'wi',\n",
       " 'whyâ',\n",
       " 'whyyyy',\n",
       " 'whyyy',\n",
       " 'whyyousoloudfor',\n",
       " 'whythreatsagainstthewest',\n",
       " 'whys',\n",
       " 'whut',\n",
       " 'wht',\n",
       " 'whoâ',\n",
       " 'whos',\n",
       " 'whores',\n",
       " 'whore',\n",
       " 'whooping',\n",
       " 'whooped',\n",
       " 'whoop',\n",
       " 'wholesome',\n",
       " 'whoisluka',\n",
       " 'whogluv',\n",
       " 'whoa',\n",
       " 'whiteâ',\n",
       " 'whitetrash',\n",
       " 'whites',\n",
       " 'whitelist',\n",
       " 'whitehouse',\n",
       " 'white',\n",
       " 'whit',\n",
       " 'whistling',\n",
       " 'whistle',\n",
       " 'whispers',\n",
       " 'whispering',\n",
       " 'whisper',\n",
       " 'whiskey',\n",
       " 'whipping',\n",
       " 'whipped',\n",
       " 'whip',\n",
       " 'whiny',\n",
       " 'whining',\n",
       " 'whiney',\n",
       " 'whine',\n",
       " 'whilst',\n",
       " 'whiff',\n",
       " 'whew',\n",
       " 'whet',\n",
       " 'whereâ',\n",
       " 'wheres',\n",
       " 'whelp',\n",
       " 'wheels',\n",
       " 'wheeling',\n",
       " 'wheelchair',\n",
       " 'wheel',\n",
       " 'wheein',\n",
       " 'wheat',\n",
       " 'whatð',\n",
       " 'whatâ',\n",
       " 'whattttttttttt',\n",
       " 'whattttt',\n",
       " 'whatttt',\n",
       " 'whattt',\n",
       " 'whattheffacts',\n",
       " 'whatsoever',\n",
       " 'whatsapp',\n",
       " 'whats',\n",
       " 'whatfeminismgaveme',\n",
       " 'whatchu',\n",
       " 'whack',\n",
       " 'whaaat',\n",
       " 'whaaaat',\n",
       " 'whaaaaat',\n",
       " 'whaaaaaat',\n",
       " 'whaaaa',\n",
       " 'whaaa',\n",
       " 'wha',\n",
       " 'wh',\n",
       " 'wgucnqsjgj',\n",
       " 'weâ',\n",
       " 'wey',\n",
       " 'wetsprocket',\n",
       " 'wethepeople',\n",
       " 'wethelgbtq',\n",
       " 'wetback',\n",
       " 'wet',\n",
       " 'westside',\n",
       " 'westhamutd',\n",
       " 'westernreviews',\n",
       " 'westerners',\n",
       " 'western',\n",
       " 'westbrook',\n",
       " 'westandwithfrance',\n",
       " 'west',\n",
       " 'wessidety',\n",
       " 'wesley',\n",
       " 'wes',\n",
       " 'werenâ',\n",
       " 'werent',\n",
       " 'weren',\n",
       " 'wentzylvania267',\n",
       " 'went',\n",
       " 'wendy',\n",
       " 'wen',\n",
       " 'wembley',\n",
       " 'welsh',\n",
       " 'welp',\n",
       " 'welly',\n",
       " 'welfare',\n",
       " 'welcoming',\n",
       " 'welcome',\n",
       " 'weirdos',\n",
       " 'weirdo',\n",
       " 'weirdest',\n",
       " 'weird',\n",
       " 'weights',\n",
       " 'weight',\n",
       " 'weigh',\n",
       " 'weezy',\n",
       " 'weezerprim',\n",
       " 'weev',\n",
       " 'weekð',\n",
       " 'weekâ',\n",
       " 'weeks',\n",
       " 'weekly',\n",
       " 'weekends',\n",
       " 'weekend',\n",
       " 'week',\n",
       " 'weedman',\n",
       " 'weed',\n",
       " 'weeb',\n",
       " 'weeauwu',\n",
       " 'wee',\n",
       " 'wednesday',\n",
       " 'wedding',\n",
       " 'wed',\n",
       " 'website',\n",
       " 'webcamâ',\n",
       " 'webcam',\n",
       " 'web',\n",
       " 'weave',\n",
       " 'weather',\n",
       " 'weasel',\n",
       " 'wears',\n",
       " 'wearing',\n",
       " 'wearin',\n",
       " 'weareoneexo',\n",
       " 'wear',\n",
       " 'weapons',\n",
       " 'weaponizedrage',\n",
       " 'weapon',\n",
       " 'wealthy',\n",
       " 'weakness',\n",
       " 'weakest',\n",
       " 'weaker',\n",
       " 'weak',\n",
       " 'wea',\n",
       " 'wdym',\n",
       " 'wcw',\n",
       " 'wbk',\n",
       " 'wba',\n",
       " 'wayâ',\n",
       " 'wayyyy',\n",
       " 'wayyy',\n",
       " 'ways',\n",
       " 'waynedupreeshow',\n",
       " 'wayne',\n",
       " 'wayment',\n",
       " 'way',\n",
       " 'wax',\n",
       " 'wavy',\n",
       " 'waving',\n",
       " 'waviest',\n",
       " 'wavesgod',\n",
       " 'waves',\n",
       " 'wave',\n",
       " 'watâ',\n",
       " 'watts',\n",
       " 'watson',\n",
       " 'wats',\n",
       " 'watfordfc',\n",
       " 'watersports',\n",
       " 'waters',\n",
       " 'watermelons',\n",
       " 'watermelon',\n",
       " 'watering',\n",
       " 'waterfall',\n",
       " 'water',\n",
       " 'watchu',\n",
       " 'watching',\n",
       " 'watchin',\n",
       " 'watches',\n",
       " 'watched',\n",
       " 'watch',\n",
       " 'watan71969',\n",
       " 'wat',\n",
       " 'wasð',\n",
       " 'wasting',\n",
       " 'wastes',\n",
       " 'wasted',\n",
       " 'waste',\n",
       " 'wassup',\n",
       " 'wasp',\n",
       " 'wasnâ',\n",
       " 'wasnt',\n",
       " 'wasn',\n",
       " 'washtimes',\n",
       " 'washingtonpost',\n",
       " 'washington',\n",
       " 'washing',\n",
       " 'washes',\n",
       " 'washedlikemelo',\n",
       " 'washed',\n",
       " 'wash',\n",
       " 'warwick',\n",
       " 'wars',\n",
       " 'warriorsialkot',\n",
       " 'warriors',\n",
       " 'warrior',\n",
       " 'warren',\n",
       " 'warning',\n",
       " 'warner',\n",
       " 'warned',\n",
       " 'warn',\n",
       " 'warmth',\n",
       " 'warming',\n",
       " 'warm',\n",
       " 'warlords',\n",
       " 'waring',\n",
       " 'warehouse',\n",
       " 'ward',\n",
       " 'warcraft',\n",
       " 'war',\n",
       " 'wants',\n",
       " 'wanting',\n",
       " 'wanted',\n",
       " 'want',\n",
       " 'wannabe',\n",
       " 'wanna',\n",
       " 'wanking',\n",
       " 'wankers',\n",
       " 'wanker',\n",
       " 'wankchat',\n",
       " 'wank',\n",
       " 'wang',\n",
       " 'wana',\n",
       " 'wan',\n",
       " 'walton',\n",
       " 'walt',\n",
       " 'walshfreedom',\n",
       " 'walmart',\n",
       " 'wallswork',\n",
       " 'walls',\n",
       " 'wallpaper',\n",
       " 'wallet',\n",
       " 'wallahi',\n",
       " 'wall',\n",
       " 'walks',\n",
       " 'walking',\n",
       " 'walkin',\n",
       " 'walker',\n",
       " 'walked',\n",
       " 'walkawayfromdemocrats',\n",
       " 'walkaway',\n",
       " 'walk',\n",
       " 'wales',\n",
       " 'wale',\n",
       " 'wal',\n",
       " 'waking',\n",
       " 'wakeupamerica',\n",
       " 'wakes',\n",
       " 'wake',\n",
       " 'wakanda',\n",
       " 'waiting',\n",
       " 'waitin',\n",
       " 'waited',\n",
       " 'wait',\n",
       " 'waist',\n",
       " 'wah',\n",
       " 'wagon',\n",
       " 'wagner',\n",
       " 'wage',\n",
       " 'waffles',\n",
       " 'waffle_gurl',\n",
       " 'waffle',\n",
       " 'wae',\n",
       " 'wadhwa',\n",
       " 'wade',\n",
       " 'waddup247',\n",
       " 'waddup',\n",
       " 'wacky',\n",
       " 'wack',\n",
       " 'wa',\n",
       " 'w_terrence',\n",
       " 'vulnerable',\n",
       " 'vulgar',\n",
       " 'vukile_vee',\n",
       " 'vs',\n",
       " 'vrob_',\n",
       " 'vro',\n",
       " 'vr',\n",
       " 'vp',\n",
       " 'voxdotcom',\n",
       " 'vows',\n",
       " 'vouch',\n",
       " 'voting',\n",
       " 'votes',\n",
       " 'voters',\n",
       " 'voterepublican',\n",
       " 'voteredtosaveamerica',\n",
       " 'votered',\n",
       " 'voter',\n",
       " 'votedemsout',\n",
       " 'voted',\n",
       " 'vote',\n",
       " 'voretaq7',\n",
       " 'vore',\n",
       " 'vonta_edition',\n",
       " 'von',\n",
       " 'vomit',\n",
       " 'voluptuous',\n",
       " 'volunteers',\n",
       " 'volunteer',\n",
       " 'voluntarily',\n",
       " 'volume',\n",
       " 'volqx',\n",
       " 'void',\n",
       " 'voices',\n",
       " 'voiceofrae',\n",
       " 'voicemail',\n",
       " 'voiced',\n",
       " 'voice',\n",
       " 'vodka',\n",
       " 'vocals',\n",
       " 'vocal',\n",
       " 'vocabulary',\n",
       " 'vlad',\n",
       " 'vizionairy',\n",
       " 'viz',\n",
       " 'vixen',\n",
       " 'vivian_games',\n",
       " 'vivek',\n",
       " 'vivafalastin',\n",
       " 'viva',\n",
       " 'viv',\n",
       " 'visually',\n",
       " 'visual',\n",
       " 'visits',\n",
       " 'visiting',\n",
       " 'visit',\n",
       " 'vision',\n",
       " 'visas',\n",
       " 'virtue',\n",
       " 'virtually',\n",
       " 'virtual',\n",
       " 'virgo',\n",
       " 'virgins',\n",
       " 'virginity',\n",
       " 'virginia',\n",
       " 'virgin',\n",
       " 'virgil',\n",
       " 'viral',\n",
       " 'vip',\n",
       " 'violetta',\n",
       " 'violent',\n",
       " 'violence',\n",
       " 'violating',\n",
       " 'violated',\n",
       " 'violate',\n",
       " 'vinyl',\n",
       " 'vintage',\n",
       " 'vincestaples',\n",
       " 'vincent',\n",
       " 'vince',\n",
       " 'villian',\n",
       " 'ville',\n",
       " 'villain',\n",
       " 'villages',\n",
       " 'village',\n",
       " 'villa',\n",
       " 'vileness',\n",
       " 'vileislam',\n",
       " 'vile',\n",
       " 'vikki',\n",
       " 'vikings',\n",
       " 'viking',\n",
       " 'views',\n",
       " 'viewers',\n",
       " 'viewer',\n",
       " 'view',\n",
       " 'vietnam',\n",
       " 'vie',\n",
       " 'vids',\n",
       " 'videos',\n",
       " 'video',\n",
       " 'vid',\n",
       " 'victorymonk',\n",
       " 'victory',\n",
       " 'victoria',\n",
       " 'victimâ',\n",
       " 'victims',\n",
       " 'victimhood',\n",
       " 'victimcard',\n",
       " 'victim',\n",
       " 'vicky',\n",
       " 'vicki',\n",
       " 'vicious',\n",
       " 'vice',\n",
       " 'vic',\n",
       " 'vibrators',\n",
       " 'vibrator',\n",
       " 'vibrating',\n",
       " 'vibing',\n",
       " 'vibin',\n",
       " 'vibez',\n",
       " 'vibes',\n",
       " 'vibehi',\n",
       " 'vibe',\n",
       " 'viagra',\n",
       " 'viaecr',\n",
       " 'vh1pnut___',\n",
       " 'vh1',\n",
       " 'vex0rian',\n",
       " 'vets',\n",
       " 'veto',\n",
       " 'veteranâ',\n",
       " 'veterans',\n",
       " 'veteran',\n",
       " 'vet',\n",
       " 'vest',\n",
       " 'versus',\n",
       " 'version',\n",
       " 'verses',\n",
       " 'verse',\n",
       " 'versacezaynx',\n",
       " 'versace',\n",
       " 'vers',\n",
       " 'veronica',\n",
       " 'vermin',\n",
       " 'verify',\n",
       " 'verified',\n",
       " 'vergil',\n",
       " 'verdict',\n",
       " 'verbally',\n",
       " 'verbal',\n",
       " 'ver',\n",
       " 'venue',\n",
       " 'vent',\n",
       " 'venom',\n",
       " 'venezuela',\n",
       " 'velvet',\n",
       " 'vele',\n",
       " 'vehicles',\n",
       " 'vehicle',\n",
       " 'veggies',\n",
       " 'veggie',\n",
       " 'vegetables',\n",
       " 'vegeta',\n",
       " 'vegas',\n",
       " 'vegans',\n",
       " 'vegan',\n",
       " 'veeh_ro',\n",
       " 'veedash_',\n",
       " 've',\n",
       " 'vc',\n",
       " 'vbxueedvn7',\n",
       " 'vault',\n",
       " 'vast',\n",
       " 'various',\n",
       " 'variety',\n",
       " 'var',\n",
       " 'vapid',\n",
       " 'vape',\n",
       " 'vanâ',\n",
       " 'vans',\n",
       " 'vanity',\n",
       " 'vanished',\n",
       " 'vanilla',\n",
       " 'vandyketrial',\n",
       " 'vandaliser',\n",
       " 'vance',\n",
       " 'van',\n",
       " 'vampire',\n",
       " 'values',\n",
       " 'value',\n",
       " 'valuable',\n",
       " 'valley',\n",
       " 'validation',\n",
       " 'valid',\n",
       " 'valentine',\n",
       " 'valenti',\n",
       " 'vain',\n",
       " 'vai',\n",
       " 'vagina',\n",
       " 'vada_fly',\n",
       " 'vacuum',\n",
       " 'vacuous',\n",
       " 'vacation',\n",
       " 'va',\n",
       " 'v_of_europe',\n",
       " 'v2cori1qmx',\n",
       " 'v2',\n",
       " 'uð',\n",
       " 'uâ',\n",
       " 'uzi',\n",
       " 'uygaraktas',\n",
       " 'uwu',\n",
       " 'utterly',\n",
       " 'utter',\n",
       " 'utd',\n",
       " 'utah',\n",
       " 'usð',\n",
       " 'usâ',\n",
       " 'usually',\n",
       " 'usual',\n",
       " 'usmovie',\n",
       " 'using',\n",
       " 'usher',\n",
       " ...]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[:100:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering <a class=\"anchor\" id=\"feature\"></a>\n",
    "\n",
    "Maybe in this part we can try to remove words (take some ideas from the final exam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelling <a class=\"anchor\" id=\"model\"></a>\n",
    "\n",
    "Try out different approaches:\n",
    "- Approach 3: Don't convert to lowercase\n",
    "- Approach 4: Include 1-grams and 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_class, X_train_dtm, X_test_dtm, y_train, y_test):\n",
    "    model = OneVsRestClassifier(model_class())\n",
    "    \n",
    "    print('Features: ', X_train_dtm.shape[1])\n",
    "    \n",
    "    model.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = model.predict(X_test_dtm)\n",
    "    \n",
    "    print('Training Accuracy: ', accuracy_score(y_train, model.predict(X_train_dtm)))\n",
    "    print('Test Accuracy: ', accuracy_score(y_test, y_pred_class))\n",
    "    \n",
    "    print('Training F1: ', f1_score(y_train, model.predict(X_train_dtm), average='macro'))\n",
    "    print('Test F1: ', f1_score(y_test, y_pred_class,  average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Base Model: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  15761\n",
      "Training Accuracy:  0.4580000482342254\n",
      "Test Accuracy:  0.41483178584348246\n",
      "Training F1:  0.5615938682318115\n",
      "Test F1:  0.5160897313132401\n"
     ]
    }
   ],
   "source": [
    "build_model(MultinomialNB, X_train_dtm, X_test_dtm, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
