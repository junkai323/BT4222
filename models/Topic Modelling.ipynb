{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lXxTIFnLBzD"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2238,
     "status": "ok",
     "timestamp": 1617597862035,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "qyOlov0TLBzD",
    "outputId": "862ee148-a3c7-4b0a-df57-598297f95243"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jun\n",
      "[nltk_data]     Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Jun\n",
      "[nltk_data]     Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Jun\n",
      "[nltk_data]     Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jun Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading files\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Data cleaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model util\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Modelling\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# nlp pre-processing\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import unidecode\n",
    "from scipy.sparse import vstack, hstack\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Z1Ah0iSMpNO"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 17227,
     "status": "ok",
     "timestamp": 1617597877066,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "Bw8qFLCPEPQ1"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('full_clean_df.xlsx')\n",
    "labels_name_list = ['NotHate', 'Racist', 'Sexist', 'Homophobe', 'Religion', 'OtherHate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "executionInfo": {
     "elapsed": 17228,
     "status": "ok",
     "timestamp": 1617597877073,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "j6KWN4k5M8pa",
    "outputId": "87f45fc6-3f1e-45dc-d693-018601c3ce8f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>NotHate</th>\n",
       "      <th>Racist</th>\n",
       "      <th>Sexist</th>\n",
       "      <th>Homophobe</th>\n",
       "      <th>Religion</th>\n",
       "      <th>OtherHate</th>\n",
       "      <th>tweets_train</th>\n",
       "      <th>tweets_emoji_train</th>\n",
       "      <th>tweets_nig_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>“NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL S...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nigga momma youngboy spit real shit nigga</td>\n",
       "      <td>nigga momma youngboy spit real shit nigga</td>\n",
       "      <td>momma youngboy spit real shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>xxsugvngxx ran holy nigga today</td>\n",
       "      <td>xxsugvngxx ran holy nigga today loudly_crying_...</td>\n",
       "      <td>xxsugvngxx ran holy  today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>“EVERYbody calling you Nigger now!” https://t....</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody call nigger</td>\n",
       "      <td>everybody call nigger</td>\n",
       "      <td>everybody call nigger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>“ real ass bitch give a fuck boutta nigga” htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>real bitch give fuck boutta nigga</td>\n",
       "      <td>real bitch give fuck boutta nigga</td>\n",
       "      <td>real bitch give fuck boutta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@WhiteHouse @realDonaldTrump Fuck ice. White s...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>fuck ice white supremacist trash racist garbage</td>\n",
       "      <td>fuck ice white supremacist trash racist garbage</td>\n",
       "      <td>fuck ice white supremacist trash racist garbage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             Tweets  NotHate  \\\n",
       "0           0  “NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL S...        1   \n",
       "1           1  RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...        1   \n",
       "2           2  “EVERYbody calling you Nigger now!” https://t....        1   \n",
       "3           3  “ real ass bitch give a fuck boutta nigga” htt...        1   \n",
       "4           4  @WhiteHouse @realDonaldTrump Fuck ice. White s...        0   \n",
       "\n",
       "   Racist  Sexist  Homophobe  Religion  OtherHate  \\\n",
       "0       0       0          0         0          0   \n",
       "1       1       0          0         0          0   \n",
       "2       1       0          0         0          0   \n",
       "3       0       0          0         0          0   \n",
       "4       1       0          0         0          1   \n",
       "\n",
       "                                      tweets_train  \\\n",
       "0        nigga momma youngboy spit real shit nigga   \n",
       "1                  xxsugvngxx ran holy nigga today   \n",
       "2                            everybody call nigger   \n",
       "3                real bitch give fuck boutta nigga   \n",
       "4  fuck ice white supremacist trash racist garbage   \n",
       "\n",
       "                                  tweets_emoji_train  \\\n",
       "0          nigga momma youngboy spit real shit nigga   \n",
       "1  xxsugvngxx ran holy nigga today loudly_crying_...   \n",
       "2                              everybody call nigger   \n",
       "3                  real bitch give fuck boutta nigga   \n",
       "4    fuck ice white supremacist trash racist garbage   \n",
       "\n",
       "                                  tweets_nig_train  \n",
       "0                    momma youngboy spit real shit  \n",
       "1                       xxsugvngxx ran holy  today  \n",
       "2                            everybody call nigger  \n",
       "3                      real bitch give fuck boutta  \n",
       "4  fuck ice white supremacist trash racist garbage  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SNmySueLBzG"
   },
   "source": [
    "### Data Dictionary <a class=\"anchor\" id=\"dict\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvPhgWVqLBzG"
   },
   "source": [
    "|Column Name|Variable Name| Description\n",
    "|---|:---:|:---\n",
    "|id|id|Unique identifier for each tweet\n",
    "|Tweets|Tweet content|Body of tweet\n",
    "|Label|classification of label|Multi-class label: sexism, racism or none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvMnmCGzLBzH"
   },
   "source": [
    "## 5. Modelling <a class=\"anchor\" id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sarsOBLoEPQ5"
   },
   "source": [
    "### No. of topics: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 274561,
     "status": "ok",
     "timestamp": 1617598134474,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "Er2PEwgaLBzH"
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# LDA = LatentDirichletAllocation(n_components=6, random_state=1)\n",
    "# LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 274559,
     "status": "ok",
     "timestamp": 1617598134475,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "ao6YoNixri1-"
   },
   "outputs": [],
   "source": [
    "# for i,topic in enumerate(LDA.components_):\n",
    "#     print(f'Top 10 words for topic #{i}:')\n",
    "#     print([vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 274557,
     "status": "ok",
     "timestamp": 1617598134476,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "E3evmMtTeJ6R"
   },
   "outputs": [],
   "source": [
    "# topic distribution for LDA\n",
    "# topic_values_LDA = LDA.transform(dtm)\n",
    "# df['topics'] = topic_values_LDA.argmax(axis=1)\n",
    "# df['topics'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 274554,
     "status": "ok",
     "timestamp": 1617598134477,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "J1vOeLd4d75E"
   },
   "outputs": [],
   "source": [
    "# df['topics'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkFDHpgTEPQ6"
   },
   "source": [
    "### No. of topics: 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 274551,
     "status": "ok",
     "timestamp": 1617598134477,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "K4b1e6u1EPQ7"
   },
   "outputs": [],
   "source": [
    "# LDA2 = LatentDirichletAllocation(n_components=7, random_state=1)\n",
    "# LDA2.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 274548,
     "status": "ok",
     "timestamp": 1617598134477,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "pISdcry0EPQ7"
   },
   "outputs": [],
   "source": [
    "# for i,topic in enumerate(LDA2.components_):\n",
    "#     print(f'Top 10 words for topic #{i}:')\n",
    "#     print([vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 274546,
     "status": "ok",
     "timestamp": 1617598134478,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "p13TNp6DEPQ7"
   },
   "outputs": [],
   "source": [
    "# topic_values_LDA2 = LDA2.transform(dtm)\n",
    "# df['topics2'] = topic_values_LDA2.argmax(axis=1)\n",
    "# df['topics2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 274543,
     "status": "ok",
     "timestamp": 1617598134478,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "uDqqWjUMEPQ7"
   },
   "outputs": [],
   "source": [
    "# df['topics2'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1n1NVh2EPQ7"
   },
   "source": [
    "### No. of topics: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 274538,
     "status": "ok",
     "timestamp": 1617598134479,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "fR_sOd1xEPQ7"
   },
   "outputs": [],
   "source": [
    "# LDA3 = LatentDirichletAllocation(n_components=5, random_state=1)\n",
    "# LDA3.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 274535,
     "status": "ok",
     "timestamp": 1617598134479,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "pnUcoPieEPQ8"
   },
   "outputs": [],
   "source": [
    "# for i,topic in enumerate(LDA3.components_):\n",
    "#     print(f'Top 10 words for topic #{i}:')\n",
    "#     print([vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 274531,
     "status": "ok",
     "timestamp": 1617598134479,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "CwnCr1WwEPQ8"
   },
   "outputs": [],
   "source": [
    "# topic_values_LDA3 = LDA3.transform(dtm)\n",
    "# df['topics3'] = topic_values_LDA3.argmax(axis=1)\n",
    "# df['topics3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 274529,
     "status": "ok",
     "timestamp": 1617598134480,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "eT5vZcu_EPQ8"
   },
   "outputs": [],
   "source": [
    "# topic_values_LDA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 274525,
     "status": "ok",
     "timestamp": 1617598134480,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "F8xC80PkEPQ8"
   },
   "outputs": [],
   "source": [
    "# df['topics3'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 274522,
     "status": "ok",
     "timestamp": 1617598134480,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "gG0xp3J5EPQ8"
   },
   "outputs": [],
   "source": [
    "# df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0DRHn3sEPQ-"
   },
   "source": [
    "### Consolidated function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 274504,
     "status": "ok",
     "timestamp": 1617598134483,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "B5z4F9rmEPQ-"
   },
   "outputs": [],
   "source": [
    "# feed cleaned tweets into this function (after initial cleaning & lemmatization & removing stopwords)\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def topic_modelling(df, tweet_column_name = 'clean_tweets', no_of_topics = 6, no_of_features = 10, \\\n",
    "                    max_df = 0.5, min_df = 10, seed = 1):\n",
    "    '''\n",
    "    Funtion input: df: Full dataframe with cleaned tweet\n",
    "                   tweet_column_name: name of column to be vectorized \n",
    "                   no_of_topics: n_component for LDA (optional)\n",
    "                   no_of_features: n_features for LDA (optional)\n",
    "                   max_df: hyperparameter for CountVectorizer (optional)\n",
    "                   min_df: hyperparameter for CountVectorizer (optional)\n",
    "                   seed: random_state (optional)\n",
    "    Funtion output: Original dataframe with cleaned tweet + probability table\n",
    "    '''\n",
    "    df[tweet_column_name] = df[tweet_column_name].astype(str) # just in case\n",
    "    \n",
    "    # tokenization default max_df = 0.5, min df =10\n",
    "    vect = CountVectorizer(max_df=0.5, min_df=10)\n",
    "    dtm = vect.fit_transform(df[tweet_column_name])\n",
    "    \n",
    "    # initialise LDA\n",
    "    LDA = LatentDirichletAllocation(n_components = no_of_topics, random_state = seed)\n",
    "    LDA.fit(dtm)\n",
    "    \n",
    "    # prints topics distribution (optional)\n",
    "    topic_modelling.topic_values_LDA = LDA.transform(dtm)\n",
    "    topics_dist = pd.DataFrame(topic_modelling.topic_values_LDA.argmax(axis=1), columns = ['topics'])\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    print(f'Topics distribution (%)')\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    print(topics_dist['topics'].value_counts(normalize = True), '\\n')\n",
    "    \n",
    "    # prints no_of_features in each topic (default features = 10)\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    print(f'Topic features (topics: {no_of_topics}, features: {no_of_features}).')\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    for i,topic in enumerate(LDA.components_):\n",
    "        print(f'Top {no_of_features} words for topic #{i}:')\n",
    "        print([vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "        print('\\n')\n",
    "    \n",
    "    # concat original df with probability table\n",
    "#     df_topics = pd.DataFrame(topic_modelling.topic_values_LDA, columns = [f'topic{str(i+1)}_prob' for i in range(topic_modelling.topic_values_LDA.shape[1])])\n",
    "    df['topics'] = topic_modelling.topic_values_LDA.argmax(axis=1)\n",
    "#     df_with_tm = pd.concat([df, df_topics], axis = 1)\n",
    "    \n",
    "#     return df_with_tm\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 576085,
     "status": "ok",
     "timestamp": 1617598436070,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "cs-f04lSEPQ_",
    "outputId": "10def2ea-a250-4ace-d0c5-e6fee86f9b41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Topics distribution (%)\n",
      "--------------------------------------------------------------------------\n",
      "3    0.347732\n",
      "5    0.173322\n",
      "2    0.155922\n",
      "1    0.116704\n",
      "0    0.116034\n",
      "4    0.090287\n",
      "Name: topics, dtype: float64 \n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "Topic features (topics: 6, features: 10).\n",
      "--------------------------------------------------------------------------\n",
      "Top 10 words for topic #0:\n",
      "['islam', 'amp', 'sexist', 'follow', 'people', 'would', 'make', 'hillbilly', 'woman', 'cunt']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['get', 'say', 'kat', 'dick', 'call', 'van', 'nigger', 'mkr', 'faggot', 'dyke']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['play', 'go', 'call', 'race', 'card', 'trash', 'white', 'fuck', 'cunt', 'retard']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['ai', 'know', 'like', 'bitch', 'real', 'fuck', 'shit', 'say', 'get', 'nigga']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['go', 'little', 'never', 'get', 'full', 'maga', 'surrender', 'redneck', 'buildthewall', 'twat']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['would', 'black', 'fuck', 'go', 'let', 'cunt', 'get', 'look', 'like', 'nigga']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = topic_modelling(df, 'tweets_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 576078,
     "status": "ok",
     "timestamp": 1617598436071,
     "user": {
      "displayName": "Jun Kai Lee",
      "photoUrl": "",
      "userId": "14518515973124114539"
     },
     "user_tz": -480
    },
    "id": "iQEQ4q-2EPQ_",
    "outputId": "a4004a86-0678-4421-ce7f-8aee10ea403a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>NotHate</th>\n",
       "      <th>Racist</th>\n",
       "      <th>Sexist</th>\n",
       "      <th>Homophobe</th>\n",
       "      <th>Religion</th>\n",
       "      <th>OtherHate</th>\n",
       "      <th>tweets_train</th>\n",
       "      <th>tweets_emoji_train</th>\n",
       "      <th>tweets_nig_train</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL S...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nigga momma youngboy spit real shit nigga</td>\n",
       "      <td>nigga momma youngboy spit real shit nigga</td>\n",
       "      <td>momma youngboy spit real shit</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>xxsugvngxx ran holy nigga today</td>\n",
       "      <td>xxsugvngxx ran holy nigga today loudly_crying_...</td>\n",
       "      <td>xxsugvngxx ran holy  today</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“EVERYbody calling you Nigger now!” https://t....</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody call nigger</td>\n",
       "      <td>everybody call nigger</td>\n",
       "      <td>everybody call nigger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“ real ass bitch give a fuck boutta nigga” htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>real bitch give fuck boutta nigga</td>\n",
       "      <td>real bitch give fuck boutta nigga</td>\n",
       "      <td>real bitch give fuck boutta</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@WhiteHouse @realDonaldTrump Fuck ice. White s...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>fuck ice white supremacist trash racist garbage</td>\n",
       "      <td>fuck ice white supremacist trash racist garbage</td>\n",
       "      <td>fuck ice white supremacist trash racist garbage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  NotHate  Racist  Sexist  \\\n",
       "0  “NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL S...        1       0       0   \n",
       "1  RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...        1       1       0   \n",
       "2  “EVERYbody calling you Nigger now!” https://t....        1       1       0   \n",
       "3  “ real ass bitch give a fuck boutta nigga” htt...        1       0       0   \n",
       "4  @WhiteHouse @realDonaldTrump Fuck ice. White s...        0       1       0   \n",
       "\n",
       "   Homophobe  Religion  OtherHate  \\\n",
       "0          0         0          0   \n",
       "1          0         0          0   \n",
       "2          0         0          0   \n",
       "3          0         0          0   \n",
       "4          0         0          1   \n",
       "\n",
       "                                      tweets_train  \\\n",
       "0        nigga momma youngboy spit real shit nigga   \n",
       "1                  xxsugvngxx ran holy nigga today   \n",
       "2                            everybody call nigger   \n",
       "3                real bitch give fuck boutta nigga   \n",
       "4  fuck ice white supremacist trash racist garbage   \n",
       "\n",
       "                                  tweets_emoji_train  \\\n",
       "0          nigga momma youngboy spit real shit nigga   \n",
       "1  xxsugvngxx ran holy nigga today loudly_crying_...   \n",
       "2                              everybody call nigger   \n",
       "3                  real bitch give fuck boutta nigga   \n",
       "4    fuck ice white supremacist trash racist garbage   \n",
       "\n",
       "                                  tweets_nig_train  topics  \n",
       "0                    momma youngboy spit real shit       3  \n",
       "1                       xxsugvngxx ran holy  today       3  \n",
       "2                            everybody call nigger       2  \n",
       "3                      real bitch give fuck boutta       3  \n",
       "4  fuck ice white supremacist trash racist garbage       2  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_topic_model = pd.concat([df, topic_modelling.df_topics], axis = 1)\n",
    "# df_topic_model.head()\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BT4222 Project (Jk).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
